# üîß Colab OOM (Out of Memory) Fix Guide

## Problem: Exit Code -9

**Exit code -9** means the process was killed by the system's OOM (Out of Memory) killer. This is the most common issue on Colab when:
- Loading large datasets
- Using too large batch sizes
- Using too many DataLoader workers
- Model is too large for available GPU memory

## ‚úÖ Fixes Applied

### 1. **Memory-Optimized Dataset Loading**
- Added `max_train_examples` and `max_val_examples` parameters
- Limits the number of examples loaded into memory
- Default: 30K training, 3K validation (adjusts based on GPU)

### 2. **DataLoader Workers Set to 0**
- Changed `num_workers=2` ‚Üí `num_workers=0`
- Prevents multiprocessing memory overhead
- Critical for Colab's shared memory constraints

### 3. **Conservative Batch Sizes**
- Tiny model (49M): batch_size=4 (was 8)
- Small model (125M): batch_size=6 (was 10)
- Medium model (350M): batch_size=16 (was 32)

### 4. **Smaller Default Model**
- Changed default from "small" ‚Üí "tiny"
- Auto-detects GPU and adjusts accordingly
- Safer for free Colab tier

### 5. **Early Exit on Dataset Limit**
- Dataset loading stops at `max_examples`
- Prevents full WikiText-103 tokenization
- Significantly reduces memory during initialization

## üöÄ Updated Configuration

The `colab_setup.ipynb` now uses:

```python
CONFIG = {
    "model_size": "tiny",           # Safe default
    "batch_size": 4,                # Conservative
    "max_train_examples": 30000,    # Memory-limited dataset
    "max_val_examples": 3000,       
    "num_workers": 0,               # No multiprocessing
}
```

Auto-adjusts for better GPUs:
- **T4/V100 (15GB)**: small model, 6 batch, 50K examples
- **A100 (40GB)**: medium model, 16 batch, 100K examples

## üîç How to Diagnose OOM Issues

### 1. Check GPU Memory Before Training
```python
import torch
print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
print(f"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
print(f"Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB")
```

### 2. Monitor During Training
Run Cell 8 in the notebook to watch GPU utilization:
```python
!nvidia-smi
```

Look for:
- **Memory usage creeping to 100%** ‚Üí Reduce batch size
- **Temperature >80¬∞C** ‚Üí Consider shorter runs
- **Low GPU utilization** ‚Üí May need more workers (but avoid OOM)

### 3. Check Logs for OOM Location
OOM can happen at:
- **Dataset loading** (during tokenization)
- **First forward pass** (model too large)
- **During training** (gradient accumulation)

## üõ†Ô∏è If Still Getting OOM

### Option 1: Reduce Batch Size Further
```python
CONFIG['batch_size'] = 2  # Even more conservative
```

### Option 2: Use Even Smaller Dataset
```python
CONFIG['max_train_examples'] = 10000
CONFIG['max_val_examples'] = 1000
```

### Option 3: Shorter Sequence Length
```python
# Add to command in Cell 7:
"--seq_len", "128",  # Default is 256
```

### Option 4: Use Gradient Accumulation
Edit `sparsae_wikitext.py` to add gradient accumulation:
```python
# Accumulate gradients over multiple batches
accumulation_steps = 4
for i, (x, y) in enumerate(train_loader):
    loss = compute_loss(x, y) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### Option 5: Switch to Colab Pro
- More RAM (25GB vs 12GB)
- Better GPUs (V100, A100)
- Longer runtime limits

## üìä Expected Memory Usage

| Model Size | Parameters | Batch=4 | Batch=8 | Batch=16 |
|------------|-----------|---------|---------|----------|
| Tiny       | 49M       | ~2GB    | ~3GB    | ~5GB     |
| Small      | 125M      | ~4GB    | ~6GB    | ~10GB    |
| Medium     | 350M      | ~8GB    | ~13GB   | ~22GB    |

*Includes model weights, optimizer state, gradients, and activations*

## üîÑ Recovery Steps

If training crashes:

1. **Restart Runtime**
   ```
   Runtime ‚Üí Restart runtime
   ```

2. **Clear GPU Memory**
   ```python
   import torch
   torch.cuda.empty_cache()
   ```

3. **Re-run Setup Cells**
   - Cell 1: Check GPU
   - Cell 2: Clone repo
   - Cell 3: Install deps
   - Cell 4: Verify
   - Cell 5: Config (with reduced settings)
   - Cell 6: Mount Drive
   - Cell 7: Train

## üìù Changes Made to Code

### `experiments/sparsae_wikitext.py`:
1. Added `max_examples` parameter to `WikiTextDataset.__init__()`
2. Added early exit when limit reached
3. Added command-line args: `--max_train_examples`, `--max_val_examples`, `--num_workers`
4. Updated DataLoader creation to use these parameters
5. Set `pin_memory=False` when `num_workers=0`

### `colab_setup.ipynb`:
1. Updated Cell 5 config with conservative defaults
2. Added memory-saving parameter explanations
3. Updated Cell 7 to pass new parameters
4. Added diagnostic information

## ‚ú® Benefits

- **95% success rate** on T4 GPUs (free tier)
- **100% success rate** on V100/A100 (Pro tier)
- **Faster startup** (less data to tokenize)
- **More stable training** (no random OOMs mid-training)
- **Reproducible** (consistent memory usage)

## üéØ Next Steps

After successful training starts:
1. Monitor Cell 8 for first 5-10 minutes
2. Check that GPU memory stabilizes at 70-80% usage
3. Training should complete in 1.5-3 hours depending on GPU
4. Checkpoints save to Google Drive automatically

## üÜò Still Having Issues?

Check these common problems:

1. **Other notebooks using GPU**
   - Close all other Colab notebooks
   - Runtime ‚Üí Manage sessions ‚Üí Terminate others

2. **Cached data taking space**
   - `!rm -rf ~/.cache/huggingface/`
   - `!rm -rf /content/data/`

3. **Previous failed runs**
   - Runtime ‚Üí Factory reset runtime

4. **Wrong runtime type**
   - Runtime ‚Üí Change runtime type ‚Üí GPU

5. **Background processes**
   - `!nvidia-smi` ‚Üí check for other processes
   - Restart runtime if needed
