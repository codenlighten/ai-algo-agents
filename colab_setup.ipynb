{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a072d4",
   "metadata": {},
   "source": [
    "# SparsAE Training on Google Colab\n",
    "\n",
    "## üéØ Quick Start Guide\n",
    "\n",
    "1. **Runtime Setup:** Runtime ‚Üí Change runtime type ‚Üí GPU (T4, V100, or A100)\n",
    "2. **Run all cells** in order\n",
    "3. **Monitor training** in the output\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Performance:\n",
    "- **T4 (Free/Pro):** 49M-125M models, ~2-3 hrs\n",
    "- **V100 (Pro):** 125M-350M models, ~1-2 hrs  \n",
    "- **A100 (Pro+):** 350M-774M models, <1 hr\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Clone your repository\n",
    "!git clone https://github.com/codenlighten/ai-algo-agents.git\n",
    "%cd ai-algo-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caeb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Install dependencies\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q transformers datasets tokenizers\n",
    "!pip install -q numpy matplotlib tqdm\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Verify installation\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úÖ Transformers: {transformers.__version__}\")\n",
    "print(f\"‚úÖ Datasets: {datasets.__version__}\")\n",
    "print(f\"‚úÖ CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb00ae3",
   "metadata": {},
   "source": [
    "## üîß Configuration\n",
    "\n",
    "Choose your experiment configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de1c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Configuration\n",
    "# Adjust these based on your GPU:\n",
    "# - T4 (15GB): model_size=\"tiny\" (49M), batch_size=8\n",
    "# - V100 (16GB): model_size=\"small\" (125M), batch_size=16\n",
    "# - A100 (40GB): model_size=\"medium\" (350M), batch_size=32\n",
    "\n",
    "CONFIG = {\n",
    "    \"model_size\": \"small\",  # \"tiny\" (49M), \"small\" (125M), \"medium\" (350M)\n",
    "    \"batch_size\": 8,\n",
    "    \"max_steps\": 10000,\n",
    "    \"sparsity\": 0.8,  # 80% sparse\n",
    "    \"save_checkpoints\": True,\n",
    "    \"checkpoint_interval\": 1000,\n",
    "}\n",
    "\n",
    "# Auto-detect optimal settings\n",
    "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f\"GPU Memory: {gpu_memory_gb:.1f} GB\")\n",
    "\n",
    "if gpu_memory_gb >= 35:  # A100\n",
    "    print(\"üöÄ Detected A100-class GPU - optimal for 350M models\")\n",
    "    if CONFIG[\"model_size\"] == \"medium\":\n",
    "        CONFIG[\"batch_size\"] = 32\n",
    "elif gpu_memory_gb >= 14:  # V100/T4\n",
    "    print(\"‚ö° Detected V100/T4-class GPU - optimal for 125M models\")\n",
    "    if CONFIG[\"model_size\"] == \"small\":\n",
    "        CONFIG[\"batch_size\"] = 12\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Limited GPU memory - recommend tiny model (49M)\")\n",
    "    CONFIG[\"model_size\"] = \"tiny\"\n",
    "    CONFIG[\"batch_size\"] = 6\n",
    "\n",
    "print(f\"\\nüìù Final Config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb7e4f",
   "metadata": {},
   "source": [
    "## üíæ Google Drive (Optional)\n",
    "\n",
    "Mount Google Drive to save checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2485a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Mount Google Drive (optional - for saving checkpoints)\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    checkpoint_dir = '/content/drive/MyDrive/sparsae_checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(f\"‚úÖ Checkpoints will be saved to: {checkpoint_dir}\")\n",
    "    \n",
    "    CONFIG['checkpoint_dir'] = checkpoint_dir\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not mount Drive: {e}\")\n",
    "    print(\"Checkpoints will be saved locally (lost on session end)\")\n",
    "    CONFIG['checkpoint_dir'] = '/content/checkpoints'\n",
    "    os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53695930",
   "metadata": {},
   "source": [
    "## üèÉ Training\n",
    "\n",
    "Run the SparsAE training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Run training\n",
    "# This will train the model with the configuration above\n",
    "\n",
    "!python experiments/sparsae_wikitext.py \\\n",
    "    --model_size {CONFIG['model_size']} \\\n",
    "    --batch_size {CONFIG['batch_size']} \\\n",
    "    --max_steps {CONFIG['max_steps']} \\\n",
    "    --sparsity {CONFIG['sparsity']} \\\n",
    "    --checkpoint_dir {CONFIG['checkpoint_dir']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc1eab0",
   "metadata": {},
   "source": [
    "## üìä Monitoring\n",
    "\n",
    "Check GPU usage during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d800e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Monitor GPU (run this in a separate cell while training)\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "for i in range(10):\n",
    "    clear_output(wait=True)\n",
    "    !nvidia-smi --query-gpu=timestamp,name,temperature.gpu,utilization.gpu,utilization.memory,memory.used,memory.total --format=csv\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6fceeb",
   "metadata": {},
   "source": [
    "## üìà Results & Download\n",
    "\n",
    "View results and download checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. View training results\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# This assumes your training script saves metrics to a CSV\n",
    "# Adjust path as needed\n",
    "try:\n",
    "    metrics = pd.read_csv('training_metrics.csv')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot training loss\n",
    "    axes[0].plot(metrics['step'], metrics['train_loss'])\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Training Loss')\n",
    "    axes[0].set_title('Training Loss Over Time')\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot validation perplexity\n",
    "    val_data = metrics[metrics['val_ppl'].notna()]\n",
    "    axes[1].plot(val_data['step'], val_data['val_ppl'])\n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_ylabel('Validation Perplexity')\n",
    "    axes[1].set_title('Validation Perplexity Over Time')\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Final Results:\")\n",
    "    print(f\"Final Training Loss: {metrics['train_loss'].iloc[-1]:.4f}\")\n",
    "    final_val_ppl = val_data['val_ppl'].iloc[-1]\n",
    "    print(f\"Final Validation Perplexity: {final_val_ppl:.2f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Metrics file not found. Training may still be running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa543853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Download checkpoints (optional)\n",
    "from google.colab import files\n",
    "\n",
    "# Compress checkpoints\n",
    "!tar -czf sparsae_checkpoints.tar.gz {CONFIG['checkpoint_dir']}\n",
    "\n",
    "print(\"üì¶ Checkpoint archive created. Download below:\")\n",
    "# Uncomment to auto-download:\n",
    "# files.download('sparsae_checkpoints.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef661ab",
   "metadata": {},
   "source": [
    "## üí° Tips & Troubleshooting\n",
    "\n",
    "### GPU Selection:\n",
    "- **Free Tier:** T4 (15GB) - Good for 49-125M models\n",
    "- **Colab Pro ($10/mo):** T4/V100 (16GB) - Good for 125M models  \n",
    "- **Colab Pro+ ($50/mo):** V100/A100 (40GB) - Good for 350M+ models\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **Out of Memory:**\n",
    "   - Reduce `batch_size` in config\n",
    "   - Use smaller model size\n",
    "   - Enable gradient checkpointing\n",
    "\n",
    "2. **Session Timeout:**\n",
    "   - Save checkpoints frequently (`checkpoint_interval=500`)\n",
    "   - Use Google Drive mount for persistence\n",
    "   - Keep browser tab active\n",
    "\n",
    "3. **Slow Training:**\n",
    "   - Check GPU utilization with `!nvidia-smi`\n",
    "   - Ensure using GPU runtime (not CPU)\n",
    "   - Increase `num_workers` if CPU-bound\n",
    "\n",
    "### Resuming from Checkpoint:\n",
    "```python\n",
    "# In the training cell, add:\n",
    "!python experiments/sparsae_wikitext.py \\\n",
    "    --resume_from /path/to/checkpoint.pt \\\n",
    "    ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Next Steps:\n",
    "\n",
    "1. **Baseline Comparisons:** Train dense, static pruning, RigL\n",
    "2. **Ablation Studies:** Test without distillation, ES, etc.\n",
    "3. **Scale Up:** Try larger models (350M) with Pro+\n",
    "4. **Paper Results:** Generate all tables and figures\n",
    "\n",
    "---\n",
    "\n",
    "**Questions?** Check the GitHub repo or open an issue!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
