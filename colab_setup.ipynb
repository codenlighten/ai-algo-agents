{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7070f155",
   "metadata": {},
   "source": [
    "# üöÄ SparsAE Training on Google Colab\n",
    "\n",
    "## Quick Start Guide\n",
    "\n",
    "1. **Runtime Setup:** Runtime ‚Üí Change runtime type ‚Üí **GPU** (T4, V100, or A100)\n",
    "2. **Run all cells** in order (Shift+Enter)\n",
    "3. **Monitor training** progress below\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Performance:\n",
    "- **T4 (Free/Pro):** 125M model in ~2-3 hrs\n",
    "- **V100 (Pro):** 125M model in ~1.5 hrs\n",
    "- **A100 (Pro+):** 350M model in ~12 hrs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è NO GPU DETECTED! Please change runtime to GPU.\")\n",
    "    print(\"Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de13630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Clone repository\n",
    "import os\n",
    "\n",
    "# Remove if already exists\n",
    "if os.path.exists('/content/ai-algo-agents'):\n",
    "    print(\"üìÅ Repository already exists, removing...\")\n",
    "    !rm -rf /content/ai-algo-agents\n",
    "\n",
    "# Clone from public GitHub (no authentication needed)\n",
    "print(\"üì• Cloning repository from GitHub...\")\n",
    "!git clone https://github.com/codenlighten/ai-algo-agents.git /content/ai-algo-agents\n",
    "\n",
    "# Change to repo directory\n",
    "%cd /content/ai-algo-agents\n",
    "\n",
    "# Verify clone successful\n",
    "print(\"\\n‚úÖ Repository cloned successfully!\")\n",
    "print(\"\\nüìÇ Repository contents:\")\n",
    "!ls -la experiments/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2105a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Install dependencies\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "!pip install -q transformers datasets tokenizers\n",
    "!pip install -q matplotlib tqdm\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730db3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Verify installation\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENVIRONMENT VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úÖ Transformers: {transformers.__version__}\")\n",
    "print(f\"‚úÖ Datasets: {datasets.__version__}\")\n",
    "print(f\"‚úÖ CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüéâ Ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2958ef4",
   "metadata": {},
   "source": [
    "## üîß Configuration\n",
    "\n",
    "Auto-detects optimal settings based on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Configuration\n",
    "import torch\n",
    "\n",
    "# Default configuration (ultra-conservative to prevent OOM)\n",
    "CONFIG = {\n",
    "    \"model_size\": \"tiny\",  # \"tiny\" (49M), \"small\" (125M), \"medium\" (350M)\n",
    "    \"batch_size\": 2,       # Ultra-conservative to prevent OOM\n",
    "    \"max_steps\": 10000,\n",
    "    \"sparsity\": 0.8,       # 80% sparse\n",
    "    \"checkpoint_interval\": 1000,\n",
    "    \"eval_interval\": 200,\n",
    "    \"max_train_examples\": 10000,  # Very limited to prevent OOM during tokenization\n",
    "    \"max_val_examples\": 1000,\n",
    "    \"num_workers\": 0,      # 0 workers to prevent multiprocessing OOM\n",
    "}\n",
    "\n",
    "# Auto-detect optimal settings based on GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GPU DETECTED: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory_gb:.1f} GB\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    if gpu_memory_gb >= 35:  # A100\n",
    "        print(\"üöÄ A100-class GPU detected!\")\n",
    "        print(\"   ‚Üí Can use larger models and batch sizes\")\n",
    "        CONFIG[\"model_size\"] = \"medium\"\n",
    "        CONFIG[\"batch_size\"] = 16\n",
    "        CONFIG[\"max_train_examples\"] = 100000\n",
    "        CONFIG[\"max_val_examples\"] = 10000\n",
    "    elif gpu_memory_gb >= 14:  # V100/T4\n",
    "        print(\"‚ö° T4/V100-class GPU detected!\")\n",
    "        print(\"   ‚Üí Can use small model with moderate settings\")\n",
    "        CONFIG[\"model_size\"] = \"tiny\"  # Start with tiny to be safe\n",
    "        CONFIG[\"batch_size\"] = 4\n",
    "        CONFIG[\"max_train_examples\"] = 20000\n",
    "        CONFIG[\"max_val_examples\"] = 2000\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Limited GPU memory detected\")\n",
    "        print(\"   ‚Üí Using minimal settings\")\n",
    "        # Keep ultra-conservative defaults\n",
    "\n",
    "print(f\"\\nüìù Final Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüí° OOM Prevention Strategy:\")\n",
    "print(f\"   ‚úÖ Ultra-low defaults (10K examples, batch=2)\")\n",
    "print(f\"   ‚úÖ Early exit during tokenization\")\n",
    "print(f\"   ‚úÖ No DataLoader workers\")\n",
    "print(f\"   ‚úÖ Progress logging every 1000 docs\")\n",
    "print(f\"\\n‚ö†Ô∏è  If still getting OOM, try:\")\n",
    "print(f\"   CONFIG['max_train_examples'] = 5000\")\n",
    "print(f\"   CONFIG['batch_size'] = 1\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5adef9b",
   "metadata": {},
   "source": [
    "## üíæ Google Drive (Optional)\n",
    "\n",
    "Mount Google Drive to save checkpoints persistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Mount Google Drive (optional)\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "try:\n",
    "    print(\"üìÅ Mounting Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    checkpoint_dir = '/content/drive/MyDrive/sparsae_checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(f\"\\n‚úÖ Google Drive mounted!\")\n",
    "    print(f\"üìÇ Checkpoints will be saved to: {checkpoint_dir}\")\n",
    "    print(\"   (These will persist after session ends)\\n\")\n",
    "    \n",
    "    CONFIG['checkpoint_dir'] = checkpoint_dir\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Could not mount Drive: {e}\")\n",
    "    print(\"Checkpoints will be saved locally (lost on session end)\\n\")\n",
    "    CONFIG['checkpoint_dir'] = '/content/checkpoints'\n",
    "    os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd15f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.5: Memory Diagnostic (run this if you get OOM errors)\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíæ MEMORY DIAGNOSTIC\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# System memory\n",
    "print(\"üìä System RAM:\")\n",
    "!free -h | grep Mem\n",
    "\n",
    "# GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_props.total_memory / 1e9\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    free = total_memory - reserved\n",
    "    \n",
    "    print(f\"\\nüìä GPU Memory ({torch.cuda.get_device_name(0)}):\")\n",
    "    print(f\"   Total:     {total_memory:.2f} GB\")\n",
    "    print(f\"   Allocated: {allocated:.2f} GB ({allocated/total_memory*100:.1f}%)\")\n",
    "    print(f\"   Reserved:  {reserved:.2f} GB ({reserved/total_memory*100:.1f}%)\")\n",
    "    print(f\"   Free:      {free:.2f} GB ({free/total_memory*100:.1f}%)\")\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\nüßπ After cleanup:\")\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    free = total_memory - reserved\n",
    "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"   Reserved:  {reserved:.2f} GB\")\n",
    "    print(f\"   Free:      {free:.2f} GB\")\n",
    "    \n",
    "    # Check for other processes\n",
    "    print(f\"\\nüîç GPU Processes:\")\n",
    "    !nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv\n",
    "    \n",
    "    # Memory recommendations\n",
    "    print(f\"\\nüí° Recommendations for {total_memory:.0f}GB GPU:\")\n",
    "    if total_memory < 12:\n",
    "        print(f\"   ‚ö†Ô∏è  Limited memory - use tiny model, batch_size=2-4\")\n",
    "        print(f\"   ‚ö†Ô∏è  Set max_train_examples=10000\")\n",
    "    elif total_memory < 16:\n",
    "        print(f\"   ‚úÖ Good for tiny model (batch_size=4-8)\")\n",
    "        print(f\"   ‚ö†Ô∏è  Small model may OOM - try batch_size=4\")\n",
    "    elif total_memory < 20:\n",
    "        print(f\"   ‚úÖ Good for small model (batch_size=6-10)\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Excellent for medium model (batch_size=16+)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected!\")\n",
    "    print(\"   Change runtime: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967fc19",
   "metadata": {},
   "source": [
    "## üèÉ Training\n",
    "\n",
    "This will train SparsAE with the configuration above. Expected time: 1.5-3 hours depending on GPU.\n",
    "\n",
    "**Before running Cell 7:** Make sure Cells 1-6 completed successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3117ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.5: Pre-training diagnostic check\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRE-TRAINING DIAGNOSTICS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Check Python\n",
    "print(f\"‚úÖ Python: {sys.executable}\")\n",
    "print(f\"   Version: {sys.version.split()[0]}\\n\")\n",
    "\n",
    "# Check working directory\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
    "print(f\"   Expected: /content/ai-algo-agents\\n\")\n",
    "\n",
    "# Check training script exists\n",
    "script_path = \"experiments/sparsae_wikitext.py\"\n",
    "if os.path.exists(script_path):\n",
    "    print(f\"‚úÖ Training script found: {script_path}\")\n",
    "    print(f\"   Size: {os.path.getsize(script_path) / 1024:.1f} KB\\n\")\n",
    "else:\n",
    "    print(f\"‚ùå Training script NOT found: {script_path}\")\n",
    "    print(f\"   Current files: {os.listdir('.')}\\n\")\n",
    "\n",
    "# Check dependencies\n",
    "print(\"üì¶ Checking dependencies:\")\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"   ‚úÖ PyTorch {torch.__version__}\")\n",
    "    print(f\"      CUDA: {torch.cuda.is_available()}\")\n",
    "except ImportError as e:\n",
    "    print(f\"   ‚ùå PyTorch: {e}\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"   ‚úÖ Transformers {transformers.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"   ‚ùå Transformers: {e}\")\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    print(f\"   ‚úÖ Datasets {datasets.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"   ‚ùå Datasets: {e}\")\n",
    "\n",
    "# Test script imports\n",
    "print(\"\\nüß™ Testing script imports...\")\n",
    "result = os.system(f\"{sys.executable} experiments/sparsae_wikitext.py --help 2>&1 | head -20\")\n",
    "if result != 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Import/help test failed with code {result}\")\n",
    "    print(\"   This may indicate missing dependencies or syntax errors\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Script imports and argument parsing OK\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"If all checks passed ‚úÖ, proceed to Cell 7\")\n",
    "print(\"If any checks failed ‚ùå, rerun cells 1-6\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a218e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Run training\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING SPARSAE TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {CONFIG['model_size'].upper()}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Max steps: {CONFIG['max_steps']}\")\n",
    "print(f\"Sparsity: {CONFIG['sparsity']*100:.0f}%\")\n",
    "print(f\"Checkpoints: {CONFIG['checkpoint_dir']}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Verify we're in the right directory\n",
    "print(f\"üìÇ Current directory: {os.getcwd()}\")\n",
    "print(f\"üìÑ Training script exists: {os.path.exists('experiments/sparsae_wikitext.py')}\\n\")\n",
    "\n",
    "# Build command with unbuffered output\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    \"-u\",  # Force unbuffered output for real-time logs\n",
    "    \"experiments/sparsae_wikitext.py\",\n",
    "    \"--model_size\", CONFIG['model_size'],\n",
    "    \"--batch_size\", str(CONFIG['batch_size']),\n",
    "    \"--max_steps\", str(CONFIG['max_steps']),\n",
    "    \"--sparsity\", str(CONFIG['sparsity']),\n",
    "    \"--checkpoint_dir\", CONFIG['checkpoint_dir'],\n",
    "    \"--checkpoint_interval\", str(CONFIG['checkpoint_interval']),\n",
    "    \"--eval_interval\", str(CONFIG['eval_interval']),\n",
    "    \"--max_train_examples\", str(CONFIG['max_train_examples']),\n",
    "    \"--max_val_examples\", str(CONFIG['max_val_examples']),\n",
    "    \"--num_workers\", str(CONFIG['num_workers']),\n",
    "]\n",
    "\n",
    "print(\"üöÄ Launching training...\")\n",
    "print(f\"üíª Command: {' '.join(cmd)}\\n\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run training with better error capture\n",
    "try:\n",
    "    result = subprocess.run(cmd, check=True, capture_output=False, text=True)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚ùå TRAINING FAILED\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Exit code: {e.returncode}\")\n",
    "    print(f\"\\nüí° Common issues:\")\n",
    "    print(f\"   - Exit code 2: Usually means argument parsing error or missing imports\")\n",
    "    print(f\"   - Check that all cells above (1-6) ran successfully\")\n",
    "    print(f\"   - Try running: !python3 experiments/sparsae_wikitext.py --help\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c04220e",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting OOM (Out of Memory)\n",
    "\n",
    "**If you got exit code -9**, the process was killed due to out of memory. Try these fixes:\n",
    "\n",
    "### Quick Fixes:\n",
    "1. **Restart runtime**: Runtime ‚Üí Restart runtime\n",
    "2. **Re-run Cell 5.5** (Memory Diagnostic) to check available memory\n",
    "3. **Reduce batch size**: In Cell 5, change `CONFIG['batch_size'] = 2`\n",
    "4. **Reduce dataset size**: In Cell 5, change `CONFIG['max_train_examples'] = 10000`\n",
    "5. **Use smaller model**: In Cell 5, change `CONFIG['model_size'] = 'tiny'`\n",
    "\n",
    "### Detailed Troubleshooting:\n",
    "See `COLAB_OOM_FIX.md` for complete guide including:\n",
    "- Memory requirements by model size\n",
    "- Gradient accumulation technique\n",
    "- Advanced optimization strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284dbd72",
   "metadata": {},
   "source": [
    "## üìä Monitoring (Optional)\n",
    "\n",
    "Run this in a separate window while training to monitor GPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f0f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Monitor GPU (run this while training runs)\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"üìä GPU Monitoring (updating every 10 seconds)\")\n",
    "print(\"Press 'Stop' button to end monitoring\\n\")\n",
    "\n",
    "try:\n",
    "    for i in range(360):  # Monitor for 1 hour\n",
    "        clear_output(wait=True)\n",
    "        print(f\"üìä GPU Status (Update {i+1}/360)\\n\")\n",
    "        !nvidia-smi --query-gpu=timestamp,name,temperature.gpu,utilization.gpu,utilization.memory,memory.used,memory.total --format=csv\n",
    "        print(\"\\n‚úÖ Healthy ranges:\")\n",
    "        print(\"   Temperature: <80¬∞C\")\n",
    "        print(\"   GPU Utilization: >90%\")\n",
    "        print(\"   Memory: <90% of total\\n\")\n",
    "        print(\"Press 'Stop' button to end monitoring\")\n",
    "        time.sleep(10)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è  Monitoring stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c668f1",
   "metadata": {},
   "source": [
    "## üìà View Results\n",
    "\n",
    "Check training progress and final metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd1cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: View results\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING RESULTS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# List checkpoints\n",
    "checkpoint_dir = CONFIG['checkpoint_dir']\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')])\n",
    "    print(f\"üìÇ Found {len(checkpoints)} checkpoints:\")\n",
    "    for cp in checkpoints[-5:]:  # Show last 5\n",
    "        path = os.path.join(checkpoint_dir, cp)\n",
    "        size_mb = os.path.getsize(path) / 1e6\n",
    "        print(f\"   {cp} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No checkpoints found yet. Training may still be running.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° To download checkpoints:\")\n",
    "print(\"   1. Go to Files panel (left sidebar)\")\n",
    "print(f\"   2. Navigate to {checkpoint_dir}\")\n",
    "print(\"   3. Right-click ‚Üí Download\")\n",
    "print(\"\\nOr run the cell below to create a compressed archive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Download checkpoints (optional)\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "checkpoint_dir = CONFIG['checkpoint_dir']\n",
    "archive_name = 'sparsae_checkpoints.tar.gz'\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(f\"üì¶ Creating compressed archive...\")\n",
    "    !tar -czf {archive_name} -C {os.path.dirname(checkpoint_dir)} {os.path.basename(checkpoint_dir)}\n",
    "    \n",
    "    size_mb = os.path.getsize(archive_name) / 1e6\n",
    "    print(f\"‚úÖ Archive created: {archive_name} ({size_mb:.1f} MB)\")\n",
    "    print(f\"\\nüì• Downloading...\")\n",
    "    \n",
    "    files.download(archive_name)\n",
    "    print(\"\\n‚úÖ Download started! Check your browser's download folder.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No checkpoints to download yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b32c89",
   "metadata": {},
   "source": [
    "## üí° Tips & Next Steps\n",
    "\n",
    "### Resuming from Checkpoint:\n",
    "If your session disconnects, rerun Cell 7 with:\n",
    "```python\n",
    "cmd.extend([\n",
    "    \"--resume_from\", \"/path/to/checkpoint_step_5000.pt\"\n",
    "])\n",
    "```\n",
    "\n",
    "### Running Experiments:\n",
    "Modify `CONFIG` in Cell 5 and rerun from there:\n",
    "- Try different sparsity levels: `0.7, 0.8, 0.9`\n",
    "- Compare model sizes: `tiny, small, medium`\n",
    "- Adjust batch size for memory/speed tradeoff\n",
    "\n",
    "### Common Issues:\n",
    "- **OOM Error:** Reduce `batch_size` in CONFIG\n",
    "- **Session Timeout:** Use Google Drive mount to save progress\n",
    "- **Slow Training:** Check GPU utilization with Cell 8\n",
    "\n",
    "### Next Steps:\n",
    "1. Run dense baseline: `CONFIG[\"sparsity\"] = 0.0`\n",
    "2. Compare results across sparsity levels\n",
    "3. Scale to larger models (upgrade to Pro+)\n",
    "4. Run ablation studies (modify training script)\n",
    "\n",
    "---\n",
    "\n",
    "**Full Documentation:** [COLAB_SETUP.md](https://github.com/codenlighten/ai-algo-agents/blob/main/COLAB_SETUP.md)\n",
    "\n",
    "**Report Issues:** [GitHub Issues](https://github.com/codenlighten/ai-algo-agents/issues)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
