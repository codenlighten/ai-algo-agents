"""
AQL v2.0 Integrated Trainer
Combines: Laplacian Uncertainty + Streaming Selection + Curriculum Learning
"""
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Optional, Dict, List
from pathlib import Path
import json
from tqdm import tqdm

# Import our modules
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from experiments.aql_v2.uncertainty.laplacian import LaplacianUncertainty
from experiments.aql_v2.data_selection.streaming_aql import StreamingAQL, SelectionBuffer
from experiments.aql_v2.curriculum.curriculum_aql import CurriculumScheduler, CurriculumAQL


class AQLv2Trainer:
    """
    Complete AQL v2.0 Training System
    
    Features:
    - Laplacian uncertainty estimation (<2% overhead)
    - Streaming data selection (memory efficient)
    - Curriculum learning (easy â†’ hard progression)
    - GPU acceleration with mixed precision
    - Comprehensive metrics tracking
    
    Usage:
        trainer = AQLv2Trainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            config=config
        )
        
        results = trainer.train(epochs=10)
    """
    
    def __init__(
        self,
        model: nn.Module,
        train_loader,
        val_loader,
        config: Dict,
        device: str = 'cuda'
    ):
        """
        Args:
            model: Neural network to train
            train_loader: Training data loader
            val_loader: Validation data loader
            config: Configuration dict with training params
            device: Device for training
        """
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        self.device = device
        
        # Initialize components
        self._setup_components()
        
        # Optimizer
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=config.get('learning_rate', 1e-4),
            weight_decay=config.get('weight_decay', 0.01)
        )
        
        # Learning rate scheduler
        self.lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config.get('total_steps', 10000)
        )
        
        # Metrics
        self.metrics = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': [],
            'selection_stats': [],
            'curriculum_stats': [],
            'uncertainty_stats': []
        }
        
        self.current_step = 0
    
    def _setup_components(self):
        """Initialize AQL v2.0 components"""
        config = self.config
        
        # 1. Laplacian Uncertainty Estimator
        self.uncertainty = LaplacianUncertainty(
            model=self.model,
            ema_decay=config.get('uncertainty_ema', 0.95)
        )
        
        # 2. Curriculum Scheduler
        self.curriculum = CurriculumScheduler(
            total_steps=config.get('total_steps', 10000),
            warmup_ratio=config.get('warmup_ratio', 0.2),
            pacing_function=config.get('pacing_function', 'root')
        )
        
        # 3. Streaming Selector (optional - for large datasets)
        if config.get('use_streaming', False):
            self.selector = StreamingAQL(
                model=self.model,
                uncertainty_estimator=self.uncertainty,
                buffer_size=config.get('buffer_size', 10000),
                selection_ratio=config.get('selection_ratio', 0.1),
                device=self.device
            )
        else:
            self.selector = None
        
        # 4. Curriculum AQL (for batch-wise selection)
        self.curriculum_aql = CurriculumAQL(
            model=self.model,
            uncertainty_estimator=self.uncertainty,
            curriculum_scheduler=self.curriculum,
            device=self.device
        )
    
    def train_epoch(self, epoch: int) -> Dict:
        """Train for one epoch"""
        self.model.train()
        
        epoch_loss = 0.0
        epoch_correct = 0
        epoch_total = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}")
        
        for batch_idx, (data, targets) in enumerate(pbar):
            data, targets = data.to(self.device), targets.to(self.device)
            
            # Apply curriculum-based selection
            if self.config.get('use_curriculum_selection', True):
                selected_data, selected_targets, _ = self.curriculum_aql.select_samples(
                    data=data,
                    targets=targets,
                    current_step=self.current_step,
                    n_select=max(len(data) // 2, 1)  # Select top 50%
                )
                
                if len(selected_data) == 0:
                    # No samples meet curriculum criteria, use all data
                    # (This happens early in training when threshold is very low)
                    pass
                else:
                    data, targets = selected_data.to(self.device), selected_targets.to(self.device)
            
            # Forward pass
            outputs = self.model(data)
            loss = nn.functional.cross_entropy(outputs, targets)
            
            # Backward pass
            self.optimizer.zero_grad()
            loss.backward()
            
            # Gradient clipping
            if self.config.get('grad_clip', 0) > 0:
                nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.config['grad_clip']
                )
            
            self.optimizer.step()
            
            # Update uncertainty estimator
            if batch_idx % self.config.get('uncertainty_update_freq', 10) == 0:
                self.uncertainty.update_fisher(data, targets)
            
            # Update LR
            self.lr_scheduler.step()
            
            # Track metrics
            epoch_loss += loss.item()
            predictions = torch.argmax(outputs, dim=1)
            epoch_correct += (predictions == targets).sum().item()
            epoch_total += len(targets)
            
            # Update progress bar
            pbar.set_postfix({
                'loss': loss.item(),
                'acc': f'{100 * epoch_correct / epoch_total:.2f}%',
                'lr': self.optimizer.param_groups[0]['lr']
            })
            
            self.current_step += 1
        
        return {
            'loss': epoch_loss / len(self.train_loader) if len(self.train_loader) > 0 else 0.0,
            'accuracy': epoch_correct / epoch_total if epoch_total > 0 else 0.0
        }
    
    @torch.no_grad()
    def validate(self) -> Dict:
        """Validate model"""
        self.model.eval()
        
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        for data, targets in self.val_loader:
            data, targets = data.to(self.device), targets.to(self.device)
            
            outputs = self.model(data)
            loss = nn.functional.cross_entropy(outputs, targets)
            
            val_loss += loss.item()
            predictions = torch.argmax(outputs, dim=1)
            val_correct += (predictions == targets).sum().item()
            val_total += len(targets)
        
        return {
            'loss': val_loss / len(self.val_loader),
            'accuracy': val_correct / val_total
        }
    
    def train(self, epochs: int) -> Dict:
        """
        Main training loop
        
        Args:
            epochs: Number of epochs to train
            
        Returns:
            Training results and metrics
        """
        print("=" * 70)
        print("AQL v2.0 Training")
        print("=" * 70)
        print(f"Model: {self.model.__class__.__name__}")
        print(f"Device: {self.device}")
        print(f"Epochs: {epochs}")
        print(f"Total steps: {self.config.get('total_steps', 'N/A')}")
        print(f"Use curriculum: {self.config.get('use_curriculum_selection', True)}")
        print(f"Use streaming: {self.config.get('use_streaming', False)}")
        print("=" * 70)
        
        best_val_acc = 0.0
        
        for epoch in range(1, epochs + 1):
            # Train
            train_metrics = self.train_epoch(epoch)
            
            # Validate
            val_metrics = self.validate()
            
            # Track metrics
            self.metrics['train_loss'].append(train_metrics['loss'])
            self.metrics['train_acc'].append(train_metrics['accuracy'])
            self.metrics['val_loss'].append(val_metrics['loss'])
            self.metrics['val_acc'].append(val_metrics['accuracy'])
            
            # Curriculum stats
            curriculum_stats = self.curriculum_aql.get_statistics(self.current_step)
            self.metrics['curriculum_stats'].append(curriculum_stats)
            
            # Uncertainty stats
            uncertainty_stats = {
                'fisher_magnitude': self.uncertainty.get_fisher_magnitude(),
                'n_updates': self.uncertainty.n_updates
            }
            self.metrics['uncertainty_stats'].append(uncertainty_stats)
            
            # Print summary
            print(f"\nEpoch {epoch}/{epochs}")
            print(f"  Train Loss: {train_metrics['loss']:.4f} | Train Acc: {train_metrics['accuracy']:.4f}")
            print(f"  Val Loss:   {val_metrics['loss']:.4f} | Val Acc:   {val_metrics['accuracy']:.4f}")
            print(f"  Curriculum Stage: {curriculum_stats['curriculum_stage']}")
            print(f"  Fisher Magnitude: {uncertainty_stats['fisher_magnitude']:.6f}")
            
            # Save best model
            if val_metrics['accuracy'] > best_val_acc:
                best_val_acc = val_metrics['accuracy']
                if self.config.get('save_path'):
                    self.save_checkpoint(
                        Path(self.config['save_path']) / 'best_model.pt',
                        epoch,
                        val_metrics['accuracy']
                    )
        
        return {
            'final_train_acc': self.metrics['train_acc'][-1],
            'final_val_acc': self.metrics['val_acc'][-1],
            'best_val_acc': best_val_acc,
            'metrics': self.metrics
        }
    
    def save_checkpoint(self, path: Path, epoch: int, accuracy: float):
        """Save model checkpoint"""
        path.parent.mkdir(parents=True, exist_ok=True)
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'accuracy': accuracy,
            'config': self.config,
            'metrics': self.metrics
        }, path)
        
        print(f"  ðŸ’¾ Saved checkpoint to {path}")
    
    def save_metrics(self, path: Path):
        """Save training metrics"""
        path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(path, 'w') as f:
            json.dump(self.metrics, f, indent=2)
        
        print(f"  ðŸ“Š Saved metrics to {path}")


def test_aql_v2_trainer():
    """Test AQL v2.0 trainer on synthetic data"""
    print("Testing AQL v2.0 Trainer")
    print("=" * 70)
    
    # Create simple model
    model = nn.Sequential(
        nn.Linear(10, 20),
        nn.ReLU(),
        nn.Linear(20, 5)
    )
    
    # Create synthetic datasets
    class SyntheticDataset(torch.utils.data.Dataset):
        def __init__(self, n_samples=1000):
            self.data = torch.randn(n_samples, 10)
            self.targets = torch.randint(0, 5, (n_samples,))
        
        def __len__(self):
            return len(self.data)
        
        def __getitem__(self, idx):
            return self.data[idx], self.targets[idx]
    
    train_dataset = SyntheticDataset(n_samples=1000)
    val_dataset = SyntheticDataset(n_samples=200)
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=32, shuffle=True
    )
    val_loader = torch.utils.data.DataLoader(
        val_dataset, batch_size=32, shuffle=False
    )
    
    # Configuration
    config = {
        'learning_rate': 1e-3,
        'weight_decay': 0.01,
        'total_steps': 100,
        'warmup_ratio': 0.2,
        'pacing_function': 'root',
        'use_curriculum_selection': True,
        'use_streaming': False,
        'uncertainty_update_freq': 5,
        'grad_clip': 1.0
    }
    
    # Create trainer
    trainer = AQLv2Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        config=config,
        device='cpu'
    )
    
    # Train
    results = trainer.train(epochs=3)
    
    print("\n" + "=" * 70)
    print("Training Complete!")
    print("=" * 70)
    print(f"Final Train Accuracy: {results['final_train_acc']:.4f}")
    print(f"Final Val Accuracy:   {results['final_val_acc']:.4f}")
    print(f"Best Val Accuracy:    {results['best_val_acc']:.4f}")
    print("\nâœ… AQL v2.0 trainer test passed!")


if __name__ == "__main__":
    test_aql_v2_trainer()
